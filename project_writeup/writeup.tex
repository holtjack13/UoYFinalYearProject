% Author: Jack Holt

% Final Year Project writeup .tex file

\documentclass{UoYCSproject}

% Package declarations
\usepackage{prftree} % Proof Trees and Natural Deduction Rules
\usepackage{listings} % Typesetting Source Code Examples

\usepackage{qtree} % Parse Trees
% Sets courier font on parse trees
\newcommand{\qlabelhook}{\ttfamily}
\newcommand{\qleafhook}{\ttfamily}

\usepackage[dvipsnames]{xcolor} % More colors

% Flow Diagrams
\usepackage{tikz}
\usetikzlibrary{arrows, shapes.geometric}

\usepackage{amsmath} %embedding normal text in math block

\usepackage{subcaption} % Used for multi-figures

\usepackage[T1]{fontenc}
\usepackage{sourcecodepro}

% Bibliography file
\addbibresource{project.bib}

% Front Matter:
\title{Generation of Type Checking Code}
\author{Jack Holt}
\date{5th March 2019}
\supervisor{Jeremy L. Jacob}
\BEng % Degree Type

\dedication{}
\acknowledgements{}

% Project Content
\begin{document}


% Make flowchart boxes used in figures
\tikzstyle{flowbox} = [rectangle, rounded corners, text centered, draw=black]
\tikzstyle{archbox} = [rectangle, text centered, draw=black, 
text width=0.25*\columnwidth, minimum height=1cm]
\tikzstyle{dashedarchbox} = [rectangle, text centered, draw=black, dashed,
text width=0.25*\columnwidth, minimum height=1cm]
\tikzstyle{label} = [text centered, text width=0.25*\columnwidth]
\tikzstyle{arrow} = [thick, ->, >=stealth]

% Instansiate front matter and other beginning pages
\maketitle
\listoffigures
\pagenumbering{roman}

% Summary
\begin{summary}
\end{summary}

\chapter{Introduction}

\section{Project Outline}

% Outline of Hypothesis and Research Question
\chapter{Background}

\section{The Role of Types in Programming Languages}
Many modern programming languages today support and encourage the use of data
types as a way of organising our data and creating abstractions with which
we can perform computation. Whether or not data types are implemented in a
programming language can be considered a design trade-off by the language
designer.

Not implementing data types would make the language less complex and,
in some cases, provide a higher level of expressibility for the programmer as
there are fewer type restrictions on each operation. Two examples of programming 
languages which don't make use of complex types are most assembly languages, 
and LISP, developed by John McCarthy in 1958. Assembly languages represent
instructions and data as numeric values, with the each instruction opcode being
applied to $n$ data operands. To improve readability, mnemonics are mapped to
each opcode, as well as key registers used by the CPU. LISP is 
known for its simplicity and can treat programs and data as one and the same 
thing because of it. Figure \ref{fig:SimpleTypedLanguages} contains some code 
snippets in both languages. The drawback of not using a more complex type system
is that the onus of checking for type inconsistencies is placed on the programmer
as this cannot be checked by the compiler in these languages.

\begin{figure}
    \begin{minipage}[b]{.45\textwidth}
        \lstset{frame=single,
            basicstyle=\footnotesize\ttfamily,
            keywordstyle=\color{Orange},
            identifierstyle=\color{MidnightBlue},
            stringstyle=\color{OliveGreen},
        }
        \lstinputlisting{code-examples/programs_and_data.rkt}
        \subcaption{LISP}
        \label{fig:LISP}
    \end{minipage} \hfill
    \begin{minipage}[b]{.5\textwidth}
        \lstset{language=[x86masm]Assembler,
            frame=single,
            basicstyle=\scriptsize\ttfamily,
            keywordstyle=\color{Orange},
            identifierstyle=\color{MidnightBlue},
            stringstyle=\color{OliveGreen},
        }
        \lstinputlisting{code-examples/add.asm}
        \subcaption{x86 Assembly}
        \label{fig:Assembly}
    \end{minipage}
    \caption{Simply Typed Languages}
    \label{fig:SimpleTypedLanguages}
\end{figure}


Introducing data types into a programming language provides its own benefits
to both the programmer and the language compiler/interpreter. By analysing the
types of each computational expression in a given program either at compile-time
or run-time, the language can assist the programmer in finding type 
inconsistencies which would result in silent erroneous calculations. This type
analysis can also be used by the compiler to allocate the optimal amount of 
space required to store the information used by the program. 

Consider the C++ program in Figure \ref{fig:C++TypeError}.
There are no syntax errors, but it is still an incorrect program due to the
``x+y" expression in Line~9. An attempt is made to add an integer to a string,
an operation whose meaning is ambiguous without further specification. This
error is caught thanks to the strong type system of C++.

Because of the prevalence of strong type systems in modern programming
languages, it could be argued that modern programmers have accepted they would
rather have better checking capabilities with less expressibility, than more
expressibility with worse checking capabilities. Hence the study of type systems
and type checking seems worthwhile and in a modern programmer's best interests.

% C++ Code Example
\begin{figure}
\lstset{language=C++,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{Orange},
    identifierstyle=\color{MidnightBlue},
    stringstyle=\color{OliveGreen},
    morecomment=[l][\color{Plum}]{\#},
    morekeywords={string},
    numbers=left,
    frame=single}

\lstinputlisting{code-examples/type_inconsistency.cpp}
\caption{Example C++ program with a type error.}
\label{fig:C++TypeError}
\end{figure}

\section{Static Semantics}
% Talk about how static semantics are usually derived from
% dynamic semantics to prove that the relationship between the two is preserved.
% For this project, not our responsibility to do this, we are trying to create
% a system which generates a type checker from specified type rules. The
% derivation of these rules is not performed by the program.
The concept of type checking falls under the study of programming language
semantics, the study of program meaning (as opposed to language syntax, which is
the study of program form). It is vital for the semantic rules of a programming
language to assign precisely one meaning to each program expressed in that
language. Failure to do so would result in the potential creation of an
ambiguous program which has multiple meanings, and may behave differently every
time it is compiled or interpreted \cite[p.~114]{Sebesta}.

Semantics can be further broken down into two kinds, static semantics and
dynamic semantics. Static semantics concerns itself with various kinds
of compile-time checks such as type checking and scope analysis, while dynamic
semantics is focused more on the program meaning described in the previous
paragraph. As this project is to do with type checking, dynamic semantics isn't
really explored in this project much. However numerous papers have been written
proposing various algorithms and methods for deriving type rules and type
systems from dynamic semantic rules \textit{<insert-references-here>}. This could
be interesting to explore in the future.

\section{Type Systems \& Type Rules}
\label{sec:Chap1TypeSystems}

% Introducing the two major views on type systems
A type system, as described by Sebesta, \textit{``is a set of types and the rules that
    govern their use in programs"} \cite[\S6, p.~309]{Sebesta}.
There are two main views on what a type system encompasses, pioneered by two
major computer scientists, Alonzo Church and Haskell B. Curry.
Church took what is known as the ``prescriptive" view of type systems, which is
the belief that \textit{``types are predefined conditions to ensure meaningfulness"}
and \textit{``for a program to even have a meaning, it must be well-typed"}
\cite{NeilJones}. Curry adopted an opposing ``descriptive" view of type systems
and believed that \textit{``any program can be executed"} and \textit{``a type
    is a way of classifying or describing the values that a program manipulates"}
\cite{NeilJones}.

% Compare the two views, and conclude that we will be following the descriptive
% view of types.
The type systems which will be generated with the generation tool will be
descriptive. We will not be considering typeless programs or expressions which
despite not having a type, still perform interesting computation (for example
the Y-Combinator in the simply typed lambda calculus)\cite{NeilJones} \cite[p.~28]{SimonPeytonJones}
\cite[p.~155]{SimonPeytonJones}.

We can express our type rules as natural deduction rules using the following
syntax:
\begin{displaymath}
    \prftree[l,r]{\scriptsize [Side-Conditions]}{\scriptsize RuleName}{J_1 \dots J_n}{C}
\end{displaymath}
where $J_k$ and $C$ are judgements. Judgements are logical statements of the form:
\begin{displaymath}
    \Gamma \vdash e:T \quad or \quad \Gamma \vdash \lfloor s \rfloor valid
\end{displaymath}
Here $e$ and $s$ represent expressions and statements respectively in the source
programming language, and $T$ represents a type in the source language. The
decision to express type rules using this style of notation stems from the desire
for this piece of work to be compatible with the BNFC tool for generating lexers
and parsers \cite{BNFC}. The notation and tool are discussed by Aarne Ranta in
his book \cite{Ranta}, and will be expanded upon in Section \ref{sec:BNFC}. An example
of a type rule in a simple language for evaluating boolean expressions could be
as follows:
\begin{displaymath}
    \prftree[l]{\scriptsize \&\&}{\Gamma \vdash a:Bool}{\Gamma \vdash b:Bool}{\Gamma \vdash a \, \&\& \, b: Bool}
\end{displaymath}
This rule reads as ``given the context $\Gamma$, the expression $a \&\& b$
is of type $Bool$ only if the expression $a$ is of type $Bool$ and the expression $b$
is of type $Bool$.

\section{Type Checking}
\label{sec:Chap1TypeChecking}
The type checker is the next component in the compiler pipeline after the
lexing/parsing unit. As an input, the type checker receives an Abstract Syntax
Tree from the parser. The tree is a representation of the syntactical structure
of the program. The type checker steps through this Abstract Syntax Tree, applying
type rules like those discussed in the previous section, to each node. If we
can construct a tree of type rule deductions in this fashion, then we have a
proof that all expressions and statements in that program are type consistent.
If however, a tree cannot be constructed, then the program is improper and
can be rejected.

Consider Figure \ref{fig:ParseAndProofTree}, which uses the
type rule specified in the previous section:
To stop the proof tree from becoming too wide, I've abbreviated $True$ to $\top$,
$False$ to $\bot$ and the type $Bool$ to $\mathbb{B}$.
% Parse and Proof Tree Example
\begin{figure}
    \begin{tikzpicture}
        \node (Code) [flowbox] {\ttfamily True \&\& False \&\& True};
        \node (ParseTree) [flowbox, below of=Code, yshift=-2cm] {\Tree [.EAnd [.EBool True ] [.EAnd [.EBool False ] [.EBool True ] ] ]};
        \node (ProofTree) [flowbox, below of=ParseTree, yshift=-3.2cm, minimum height=2.8cm, minimum width=12.5cm] {
            \prftree[l]{\scriptsize \&\&}{
                \prftree[l]{\scriptsize trueLiteral}{\Gamma \vdash \top:\mathbb{B}}}{
                \prftree[l]{\scriptsize \&\&}{
                    \prftree[l]{\scriptsize falseLiteral}{\Gamma \vdash \bot:\mathbb{B}}}{
                    \prftree[l]{\scriptsize trueLiteral}{\Gamma \vdash \top:\mathbb{B}}}{
                    \Gamma \vdash \bot \, \&\& \, \top: \mathbb{B}}}{
                \Gamma \vdash \top \, \&\& \, (\bot \, \&\& \, \top): \mathbb{B}}
            };
            \draw [arrow] (Code) -- node [anchor=west] {\scriptsize when parsed becomes} (ParseTree);
            \draw [arrow] (ParseTree) -- node [anchor=west] {\scriptsize is proven to be well-typed by} (ProofTree);
        \end{tikzpicture}
    \caption{A abstract syntax tree for a simple boolean expression, and its corresponding proof tree}
    \label{fig:ParseAndProofTree}
\end{figure}
Notice how the proof tree mirrors the structure of the Abstract Syntax Tree.
Each type rule application happens at a node in the parse tree.

\section{Generating Type Checkers vs. Generating Lexers \& Parsers}
The mathematical tools of choice for a compiler designer looking to design
a Lexer or Parser for their language are the Finite State Automaton and the
Pushdown Automaton. These automata are well understood by the computer science
community, so much so that computer programs exist which can take descriptions
of these machines in the form of Regular Expressions and Context-Free Grammars
and produce the machines they describe as an executable program. These programs
are incredibly useful for compiler designers, and speed up the design and
implementation phases of compiler construction. Examples of such Lexer and Parser
generators are Alex and Happy, which are employed by the BNFC tool.

Whilst the methods of expressing Lexers and Parsers seems to be agreed on by
many, the best way of describing a Type System or a Static Semantics is still
debated \cite{NeilJones}.

% Exploration of currently available tools and key texts and literature
\chapter{Literature Review}

\section{Backus-Naur Form Converter (BNFC)}
\label{sec:BNFC}

The Backus-Naur Form Converter (BNFC) is a tool for generating lexer and parser
compiler units. It takes as an input a Labelled BNF grammar \cite{LBNFReport},
in the form of a {\ttfamily .cf} file, and produces a set of source code files
which, when used together, can parse the source language described in the
grammar. Given some input language specification {\ttfamily source\_lang.cf},
a few important output files are produced. A description is provided for each
file:
\begin{itemize}
    \item {\ttfamily AbsSourceLang.hs} - a Haskell module containing the Abstract
        Data Types for the nodes of the Abstract Syntax Tree built by the
        parser
    \item {\ttfamily LexSourceLang.x} - an Alex file containing a description
        of the lexer for the source language
    \item {\ttfamily ParSourceLang.y} - a Happy file containing a description
        of the parser for the source language
    \item {\ttfamily ErrM.hs} - a Haskell module describing a Monad for handling
        errors when parsing the source language (Monads will be discussed more
        in Section \ref{sec:CategoryTheory})
    \item {\ttfamily Makefile} - a Makefile which runs Alex and Happy on the
        lexer and parser respectively, and compiles and links TestSourceLang.hs
        to create an executable
\end{itemize}

The University of York Computer Science department makes use of the BNFC tool
in the Implementation of Programming Languages (IMPL) course, where students
have the opportunity to investigate various components of the compiler pipeline.
Because BNFC allows students to rapidly prototype language ideas, and the files
generated can be utilised to produce fully working compilers, the tool is great
for learning and for small projects.

While writing the lexer and parser components for the type system markup language
myself might result in a more efficient and complete product, I will instead be
making use of the BNFC tool to create the front end components. This is not only
because of the pros of the tool discussed above, but also due to the timescale
of the project. Attempting to construct the front-end components alongside
completing my other 3rd year modules simply wouldn't be viable. Using the BNFC
tool also provides a more complicated LBNF example for IMPL students to
investigate in future years of study.

\section{Attribute Grammars \& Syntax Directed Translation}
We described type checking in the previous chapter, but how do we
integrate it into the compiler pipeline? One method, conceived by Donald Knuth
is the Attribute Grammar \cite{KnuthGrammars}. An attribute grammar is a
context-free grammar which has been extended to include additional semantic
rules, tied to each non-terminal symbol. These additional rules are known as
attributes. Sebesta provides a nice example of an attribute grammar for an
assignment statement in a simple language, shown in Figure 
\ref{fig:AttributeGrammar}.

\begin{figure}
    \begin{minipage}[b]{.5\textwidth}
        \begin{align*}
            Assign &\rightarrow Var \; "=" \; Expr \\
            Expr &\rightarrow Var_2 "+" Var_3 \\
            Expr &\rightarrow Var \\
            Var &\rightarrow A \, | \, B \, | \, C
        \end{align*}
        \subcaption{Production Rules}
        \label{fig:ProductionRules}
    \end{minipage}
    \begin{minipage}[b]{.5\textwidth}
        \begin{align*}
                ExpectedType(Expr) &= ActualType(Var) \\
                ActualType(Expr) &= if ActualType(Var_2 = Int) \&\& \\
                ActualType(Expr) &= ActualType(Var) \\
                ActualType(Var) &= lookupType(Var)
        \end{align*}
        \subcaption{Attribute Definitions}
        \label{fig:AttributeDefinitions}
    \end{minipage}
    \caption{An attribute grammar for an assignment statement}
    \label{fig:AttributeGrammar}
\end{figure}

\textit{Ignore the example above for now, it's incorrect. I'm working on creating
    an example attribute grammar which fits the context of the project
    appropriately.}

\textit{This section will be completed as soon as possible}

\section{Advanced Type Rules \& Context}
With attribute grammars annotating the program abstract syntax tree with type
information using attributes, an alternative approach involves processing the
abstract syntax tree using pattern matching functions in a functional programming
language like Haskell. These functions would implement type rules like those
described in Section \ref{sec:Chap1TypeSystems} and \ref{sec:Chap1TypeChecking}.
To begin thinking about how we might construct these functions, we should first
consider what form they might take, and whether the form is predictable enough
to generate.

\subsection{Representing Simple Type Rules in a Programming Language}
Consider the following type rule:
\begin{displaymath}
    \prftree[l]{||}{\Gamma \vdash a:\mathbb{B}}{\Gamma \vdash b:\mathbb{B}}{
    \Gamma \vdash a \, || \, b:\mathbb{B}}
\end{displaymath}
Rules like the one above are easy to translate into
Haskell and seem to have a predictable form which can be generated. Consider
the following Haskell source code:
\lstset{language=Haskell,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{Orange},
    identifierstyle=\color{MidnightBlue},
    stringstyle=\color{OliveGreen},
    showstringspaces=false,
    frame=single}
\lstinputlisting{code-examples/and_type_rule.hs} 

The consequents of the rule appear as arguments to the check function on the
left hand side, and the antecedents appear as the function body on the right
hand side. The TrueLiteral and FalseLiteral rules return \lstinline{Ok True}
because they are axioms and have no antecedents. Type rules which don't mutate
the program context should be easy to generate because they don't require us
to lookup the types of any variables. The complexity of our type rule
implementations will begin to develop when we need to start considering the
program context when type checking.

\subsection{Context, Environment \& Scoping}
If we are type checking an expression that is incredibly simple and only uses
literal values such as True, False and 42, then we won't need to consult the
program context. Unfortunately, programming languages which only make use of
literal types don't gain much traction in the real world because they aren't
very useful. Modern languages make use of more powerful language concepts such
as variables and functions, concepts which require us to keep track of
declarations and definitions in order to type check our programs correctly.
This is precisely what the context structure is for, to store variable-type
pairs or function type-pairs.

The design and behaviour of the context data structure used for storing these
bindings is largely dictated by the scoping rules of the programming language.
If the language decided to make use of static scoping, then the contexts
structures may be pushed and popped from a context stack as new blocks are
entered and exited. If dynamic scoping were used instead, then a different
implementation would be required. The designer of the type system would need
to describe the behaviour of the context data structure in the type system
language to make the type checker generator as flexible as possible.

Ranta \cite{Ranta} proposes a model for contexts appropriate for a statically
scoped langauge in the form of the following Haskell source code:
\lstinputlisting{code-examples/context.hs}
The code describes the context as a pairing of a hash map for mapping function
names to their corresponding input and output types, and a list of hash maps
mapping variable names to types. The function declarations remain global and can
be seen as they are acquired by the type checker. The contexts can be pushed
and popped from the context stack as they are encountered when traversing the
parse tree.

\subsection{More Advanced Type Rules}
Now consider the following type rule for type checking a variable, and its
corresponding code:
\begin{displaymath}
    \prftree[l,r]{[if $v:T$ in $\Gamma$]}{VarT}{\Gamma \vdash v:T}
\end{displaymath}

\lstinputlisting{code-examples/var_type_rule.hs}
This rule seems more complicated to express in a programming language, as it
requires us to check whether we have a variable binding in our context which
matches the variable we are checking. Looking up a variable would be considered
a side-condition in a type rules, and it seems that any rules which have
side-conditions will be harder to generate.

Here are some examples of rules with different side-conditions, as documented
by Pierce \cite{PierceTAPL}:

\begin{displaymath}
\prftree[l, r]{[if $f:(T_1, \dots, T_n) \rightarrow T$ in $\Gamma$]}{FuncCallT}{\Gamma \vdash a_1:T_1}{\dots}{\Gamma \vdash a_n:T_n}{\Gamma \vdash f(a_1, \dots, a_n):T}
\end{displaymath}
\begin{displaymath}
\prftree[l, r]{[if Pointer T]}{LocationT}{\Gamma \vdash l:Pointer \, T}
\end{displaymath}

\textit{This section will be fleshed out to include type rules for more complex
    types such as pairs, product types, records, sum types, lists,
    pointers, etc...}


\section{Applying Category Theory To Type Checking}
\label{sec:CategoryTheory}

Using a functional language like Haskell provides us with the capability to
create elegant abstractions with which we can perform powerful computations
with. By borrowing some ideas from Category Theory, we can enhance our Abstract
Syntax data structures with desirable properties to make type checker generation
easier.

\subsection{Functors}
The first abstraction we will borrow from Category Theory is the Functor. The
use of higher order functions is becoming ever more prevalent in modern
programming languages, especially functions such as \lstinline{map},
\lstinline{filter} and \lstinline{foldr}. The \lstinline{map} function in
particular is incredibly powerful, as it can take a function and a list as
input, and repeatedly apply that function to every element in the list returning
a new list as a result. The following Haskell code provides an implementation
of \lstinline{map}:
\lstinputlisting{code-examples/map.hs}
From looking at the function signature, it can be seen that map only works on
lists. But what if we wanted to apply map to a data structure other than list?
The Functor allows us to do this. A data structure is considered a Functor if
it provides an implementation of the \lstinline{fmap} function, with the
following type signature:
\lstinputlisting{code-examples/fmap.hs}
Here, \lstinline{f} is our data structure. The \lstinline{fmap} function
generalises the map function to some data structure \lstinline{f}. An example
of this concept being used specifically in this project could be...

\textit{I'm working on a suitable example for this section...}

\subsection{Monads}
We can extend the concept of the Functor further to make use of another
abstraction known as a Monad. A Monad is a data structure which implements the
following two operations, described here in Haskell:
\lstinputlisting{code-examples/monad.hs}
Monads provide us with a way of seperating pure and impure computation which
may have side-effects. A very useful application of this is in error handling
and performing operations which may or may not fail. If we find when type
checking that our program fails, we should attempt to diagnose the problem and
report back to the user. A data structure for this very purpose is generated by
the BNFC tool and stored in the {\ttfamily ErrM.hs} file mentioned in Section
\ref{sec:BNFC}, shown below:
\lstinputlisting{code-examples/err.hs}
If the computation is successful then we can return some value of type
{\ttfamily a}, otherwise we return a string which explains the error. The
{\ttfamily >>=} operator provides us with to chain together computations using
the {\ttfamily Err} data type. If at any point the computation produces an
error, we can simply propagate the error message to the end of the computation
chain. Consider the following type checking code:
%\lstinputlisting{code-examples/monad_type_check.hs}

\textit{Working on a do-notation example demonstrating the Err data structure...}

\subsection{Foldables}

\cite{MilewskiCTFP}

\section{Exploring a Previous Example: Typical}
\cite{Typical}

\chapter{Developing a Type Rule Plaintext Syntax}

\section{The Bigger Picture}

The end product we are creating requires us to take into account many different
software components. To better understand these components and how they depend
on each other, we can construct an component diagram to help us visualise
the relationships. The diagram is shown in Figure \ref{fig:SoftwareComponents}.
The components this project focuses on are highlighted in red. This chapter
will focus on the development of the Type Language Specification and the 
Type Rule Lexer/Parser generated from it, while Chapter 5 will focus on the 
Type Checker Generator.

\begin{figure}
    \begin{tikzpicture}
        \node (SLSC) [label] {Source Language Source Code};
        \node (TLS) [label, above of=SLSC, color=red, yshift=3cm] {Type System Language Specification};
        \node (SLL/P) [archbox, right of=SLSC, xshift=3cm] {Source Language Lexer/Parser};
        \node (SLTC) [archbox, right of=SLL/P, xshift=3cm] {Source Language Type Checker};
        \node (TCG) [archbox, above of=SLTC, yshift=1cm, color=red] {Type Checker Generator};
        \node (BNFC) [archbox, above of=SLL/P, yshift=3cm] {BNFC};
        \node (TCGL/P) [archbox, above of=TCG, yshift=1cm, xshift=2cm, color=red] {Type Rule Lexer/Parser};
        \node (CBE) [dashedarchbox, right of=SLTC, xshift=3cm] {Compiler Backend};
        \node (TR) [label, above of=TCGL/P, yshift=1cm] {Type Rule Specification};
        \node (SLS) [label, above of=BNFC, yshift=1cm] {Source Language Specification};

        \draw [arrow] (SLS) -- (BNFC);
        \draw [arrow] (TR) -- (TCGL/P);
        \draw [arrow] (SLSC) -- (SLL/P);
        \draw [arrow] (TLS) -- (BNFC);
        \draw [arrow] (BNFC) -- node [anchor=east] {\footnotesize generates}(SLL/P);
        \draw [arrow] (BNFC) -- node [anchor=south] {\footnotesize generates}(TCGL/P);
        \draw [arrow] (SLL/P) -- (SLTC);
        \draw [arrow] (TCG) -- node [anchor=west] {\footnotesize generates} (SLTC);
        \draw [arrow] (TCGL/P) -- (TCG);
        \draw [arrow] (SLTC) -- (CBE);
        \draw [arrow] (SLS) .. controls +(up:8cm) and +(right:12cm) .. (TCG);
    \end{tikzpicture}

    \caption{A diagram relating the various components of the source language with
        the type system language}
    \label{fig:SoftwareComponents}
\end{figure}


\section{Initial Language Design and Ideas}


\section{Shuffling Syntax and Eliminating Dependencies}

\chapter{Type Checker Generation}

\chapter{Results and Evaluation}



\printbibliography
\end{document}
