% Author: Jack Holt

% Final Year Project writeup .tex file

\documentclass{UoYCSproject}

% Package declarations
\usepackage{prftree} % Proof Trees and Natural Deduction Rules
\usepackage{listings} % Typesetting Source Code Examples

% Slanted Fractions
\usepackage{xfrac}

\usepackage{qtree} % Parse Trees
% Sets courier font on parse trees
\newcommand{\qlabelhook}{\small\ttfamily}
\newcommand{\qleafhook}{\small\ttfamily}

\usepackage[dvipsnames]{xcolor} % More colors

% Flow Diagrams
\usepackage{tikz}
\usetikzlibrary{arrows, shapes.geometric}

\usepackage{amsmath} %embedding normal text in math block

\usepackage{subcaption} % Used for multi-figures

\usepackage{amsthm}

% Nice code font
\usepackage[T1]{fontenc}
\usepackage{sourcecodepro}

% Define theorem environment
\newtheorem{definition}{Definition}[section]

\newtheorem*{hypothesis}{Hypothesis}
\newtheorem*{researchquestion}{Research Question}

% Bibliography file
\addbibresource{project.bib}

% Front Matter:
\title{Generation of Type Checking Code}
\author{Jack Holt}
\date{25th April 2019}
\supervisor{Jeremy L. Jacob}
\BEng % Degree Type

\acknowledgements{I would like to thank my supervisor, Jeremy Jacob, for his
    continued assistance and informative feedback over the course of this
    project. I would also like to thank my family for their support when I've
    needed it most, and my friends for making the journey to this point a
    memorable one.}

% Project Content
\begin{document}

% Make flowchart boxes used in figures
\tikzstyle{flowbox} = [rectangle, rounded corners, text centered, draw=black]
\tikzstyle{archbox} = [rectangle, text centered, draw=black,
text width=0.25*\columnwidth, minimum height=1cm]
\tikzstyle{dashedarchbox} = [rectangle, text centered, draw=black, dashed,
text width=0.25*\columnwidth, minimum height=1cm]
\tikzstyle{label} = [text centered, text width=0.25*\columnwidth]
\tikzstyle{arrow} = [thick, ->, >=stealth]

% Instantiate front matter and other beginning pages
\maketitle
\listoffigures
\pagenumbering{roman}

% Summary
\begin{summary}
\end{summary}

\chapter{Introduction}

\section{Project Outline}
This project is an investigation into the feasibility of type checker generation,
through the attempted construction of a type system description language, and a
generic type checker generator. The generator, when given a language specification 
and a set of type rules written in the type system language, will produce a 
type checker enforcing the described type system. Several challenges were faced 
when building the software, with the hardest issues to solve relating to the 
difficulty of keeping the type system language generic.


\section{Report Structure}
This report is structured into chapters as follows:
\begin{enumerate}
    \item Background: A section explaining the required pre-requisite topics
        before more complex ideas and methods are introduced
    \item Literature Review: A section exploring methods of type checking,
        functional programming and its suitability to type checker generation, 
        and advanced type rules
    \item Problem Analysis: 
    \item Solution Design:
    \item Implementation: A section discussing the software implementation of
        the language and generator
    \item Results \& Evaluation: A discussion of the results and the success
        of the project
\end{enumerate}

\chapter{Background}

\section{The Role of Types in Programming Languages}
Many modern programming languages today support and encourage the use of data
types as a way of organising our data and creating abstractions with which
we can perform computation. Whether or not data types are implemented in a
programming language can be considered a design trade-off by the language
designer.

Not implementing data types could make the implementation of a language easier
or more difficult relative to typed languages, depending on the priority of
language safety. In order for a untyped language to be considered safe, as
defined by Cardelli \cite[p.~3]{CSHandbook}, run-time checks would need to be added
to check for \textit{untrapped errors}, such as incorrect memory jumps and
reading memory past the bounds of an array. These checks could result in
decreased run-time performance of programs. LISP, developed by John McCarthy in
1958, is considered to be an untyped safe language, whilst most assembly
languages are classified as untyped and unsafe. Typed languages can usually
eliminate the need for many run-time checks by performing them at compile-time.

In some cases untyped languages provide a higher level of expressibility for
the programmer, as there are fewer type restrictions on each operation.
LISP is known for its expressive power due to its ability to treat programs and
data as one and the same thing. A consequence of this is the ease of which
powerful programming techniques such as meta-programming can be utilised to
produce domain specific languages designed for solving a distinct class of
problems \cite[\S1, p.~5]{SICP}. Figure \ref{fig:SimpleTypedLanguages} contains a
simple LISP example showcasing its syntax.

\begin{figure}
    \lstset{frame=single,
        language=Lisp,
        numbers=left,
        basicstyle=\footnotesize\ttfamily,
        keywordstyle=\color{Orange},
        identifierstyle=\color{MidnightBlue},
        stringstyle=\color{OliveGreen},
    }
    \lstinputlisting{code-examples/programs_and_data.rkt}
    \caption{A LISP example}
    \label{fig:SimpleTypedLanguages}
\end{figure}

Introducing static typing into a programming language provides its own benefits
to both the programmer and the language compiler/interpreter. By analysing the
types of each computational expression in a given program at compile-time,
the language can assist the programmer in finding type
inconsistencies which would result in silent erroneous calculations. This type
analysis can also be used by the compiler to allocate the optimal amount of
space required to store the information used by the program.

Consider the C++ program in Figure \ref{fig:C++TypeError}.
There are no syntax errors, but it is still incorrect due to the
``x+y" expression in Line~9. An attempt is made to add an integer to a string,
an operation whose meaning is ambiguous without further specification. This
error is caught at compile-time thanks to the strong type system of C++.

Because of the prevalence of strong type systems in modern programming
languages, it could be argued that modern programmers have accepted they would
rather have better checking capabilities with less expressibility, than more
expressibility with worse checking capabilities. Hence the study of type systems
and type checking seems worthwhile and in a modern language designer's best
interest.

% C++ Code Example
\begin{figure}
\lstset{language=C++,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{Orange},
    identifierstyle=\color{MidnightBlue},
    stringstyle=\color{OliveGreen},
    morecomment=[l][\color{Plum}]{\#},
    morekeywords={string},
    numbers=left,
    frame=single}

\lstinputlisting{code-examples/type_inconsistency.cpp}
\caption{Example C++ program with a type error.}
\label{fig:C++TypeError}
\end{figure}

\section{Static Semantics}
% Talk about how static semantics are usually derived from
% dynamic semantics to prove that the relationship between the two is preserved.
% For this project, not our responsibility to do this, we are trying to create
% a system which generates a type checker from specified type rules. The
% derivation of these rules is not performed by the program.
The concept of type checking falls under the study of programming language
semantics, the study of program meaning (as opposed to language syntax, which is
the study of program form) \cite[\S3.1, p.~114]{Sebesta}. It is vital for the 
semantic rules of a programming language to assign precisely one meaning to 
each program expressed in that language. Failure to do so would result in the 
potential creation of an ambiguous program which has multiple meanings, and may 
behave differently every time it is compiled or interpreted 
\cite[\S3.3.1.7 ,p.~123]{Sebesta}.

Semantics can be further broken down into two kinds, static semantics and
dynamic semantics. Static semantics concerns itself with various kinds
of compile-time checks such as type checking and scope analysis. Mosses
describes static semantics as the \textit{``checking for well-formedness"} of
programs before they are compiled and executed \cite{Mosses}. Dynamic
semantics is focused more on the program meaning described in the previous
paragraph, and requires the program to be executed in order for that meaning to
be determined.

As this project is focused on type checking, dynamic semantics
isn't explored here. However a few papers have been written
proposing various algorithms and methods for deriving type rules and type
systems from dynamic semantic rules \cite{NeilJones} \cite{JohnHannan}. This could
be an interesting avenue to explore in future projects relating to type checkers.

\section{Type Systems \& Type Rules}
\label{sec:Chap1TypeSystems}

% Introducing the two major views on type systems
A type system, as described by Sebesta, \textit{``is a set of types and the rules that
    govern their use in programs"} \cite[\S6.15, p.~309]{Sebesta}.
There are two main views on what a type system encompasses, pioneered by two
major computer scientists, Alonzo Church and Haskell B. Curry.
Church took what is known as the ``prescriptive" view of type systems, which is
the belief that \textit{``types are predefined conditions to ensure meaningfulness"}
and \textit{``for a program to even have a meaning, it must be well-typed"}
\cite{NeilJones}. Curry adopted an opposing ``descriptive" view of type systems
and believed that \textit{``any program can be executed"} and \textit{``a type
    is a way of classifying or describing the values that a program manipulates"}
\cite{NeilJones}.

% Compare the two views, and conclude that we will be following the descriptive
% view of types.
The type systems which will be generated with the generation tool will be
descriptive. We will not be considering typeless programs or expressions which
despite not having a type, still perform interesting computation (for example
the Y-Combinator in the simply typed lambda calculus) \cite{NeilJones} 
\cite[p.~28]{SimonPeytonJones} \cite[p.~155]{SimonPeytonJones}.

We can express our type rules as natural deduction rules using the following
syntax:
\begin{displaymath}
    \prftree[l,r]{\scriptsize [Side-Conditions]}{\scriptsize RuleName}{J_1 \dots J_n}{C}
\end{displaymath}
where $J_k$ for $k=1, \dots ,n$ and $C$ are judgements. Judgements are logical 
statements of the form:
\begin{displaymath}
    \Gamma \vdash \mathbb{A}
\end{displaymath}
Here, $\mathbb{A}$ represents an assertion which is made about some object and
its type. $\Gamma$ represents the environment (or context as it's sometimes
referred to) in which the assertion in evaluated. This notation is used by both
Cardelli \cite[p.~8]{CSHandbook} and by Ranta \cite[\S4.4, p.~60]{Ranta} to 
describe judgements.

The decision to express type rules using this style of notation stems from the 
desire to maintain a common language of type rules for readers of Ranta's key
text \textit{``Implementing Programming Languages"} \cite{Ranta}, which is a key 
text for learning the BNFC tool \cite{BNFC}. The BNFC tool is used throughout
this project, with a detailed breakdown of it taking place in Section 
\ref{sec:BNFC}. 

An example of a type rule in a simple language for evaluating boolean 
expressions is as follows:
\begin{displaymath}
    \prftree[l]{\scriptsize \&\&}{\Gamma \vdash a:Bool}{\Gamma \vdash b:Bool}{\Gamma \vdash a \, \&\& \, b: Bool}
\end{displaymath}
This rule reads as ``given any context $\Gamma$, the expression $a \, \&\& \, b$
is of type $Bool$ only if the expression $a$ is of type $Bool$ and the expression $b$
is of type $Bool$.

\section{Type Checking}
\label{sec:Chap1TypeChecking}
The type checker is the next component in the compiler pipeline after the
lexing/parsing unit. As an input, the type checker receives an Abstract Syntax
Tree from the parser. The tree is a representation of the syntactical structure
of the program. The type checker steps through this Abstract Syntax Tree, applying
type rules like those discussed in the previous section, to each node. If we
can construct a tree of type rule deductions in this fashion, then we have a
proof that all expressions and statements in that program are type consistent.
If however, a tree cannot be constructed, then the program is improper and
can be rejected.

Consider Figure \ref{fig:ParseAndProofTree}, which uses the
type rule specified in the previous section:
To stop the proof tree from becoming too wide, I've abbreviated $True$ to $\top$,
$False$ to $\bot$ and the type $Bool$ to $\mathbb{B}$.
% Parse and Proof Tree Example
\begin{figure}
    \begin{tikzpicture}
        \node (Code) [flowbox] {\ttfamily True \&\& False \&\& True};
        \node (ParseTree) [flowbox, below of=Code, yshift=-2cm] {\Tree [.EAnd [.EBool True ] [.EAnd [.EBool False ] [.EBool True ] ] ]};
        \node (ProofTree) [flowbox, below of=ParseTree, yshift=-3.2cm, minimum height=2.8cm, minimum width=12.5cm] {
            \prftree[l]{\scriptsize \&\&}{
                \prftree[l]{\scriptsize trueLiteral}{\Gamma \vdash \top:\mathbb{B}}}{
                \prftree[l]{\scriptsize \&\&}{
                    \prftree[l]{\scriptsize falseLiteral}{\Gamma \vdash \bot:\mathbb{B}}}{
                    \prftree[l]{\scriptsize trueLiteral}{\Gamma \vdash \top:\mathbb{B}}}{
                    \Gamma \vdash \bot \, \&\& \, \top: \mathbb{B}}}{
                \Gamma \vdash \top \, \&\& \, (\bot \, \&\& \, \top): \mathbb{B}}
            };
            \draw [arrow] (Code) -- node [anchor=west] {\scriptsize when parsed becomes} (ParseTree);
            \draw [arrow] (ParseTree) -- node [anchor=west] {\scriptsize is proven to be well-typed by} (ProofTree);
        \end{tikzpicture}
    \caption{A abstract syntax tree for a simple boolean expression, and its corresponding proof tree}
    \label{fig:ParseAndProofTree}
\end{figure}
Notice how the proof tree mirrors the structure of the Abstract Syntax Tree.
Each type rule application happens at a node in the parse tree.

\section{Generating Type Checkers vs. Generating Lexers \& Parsers}
The mathematical tools of choice for a compiler designer looking to design
a Lexer or Parser for their language are the Finite State Automaton and the
Pushdown Automaton. These automata are well understood by the computer science
community, so much so that computer programs exist which can take descriptions
of these machines in the form of Regular Expressions and Context-Free Grammars
and produce the machines they describe as an executable program. These programs
are incredibly useful for compiler designers, and speed up the design and
implementation phases of compiler construction. Examples of such Lexer and Parser
generators are Alex and Happy, which are employed by the BNFC tool.

Whilst the methods of expressing Lexers and Parsers seems to be agreed on by
many, the best way of describing a Type System or a Static Semantics is still
debated \cite{NeilJones}.

% Exploration of currently available tools and key texts and literature
\chapter{Literature Review}

\section{Backus-Naur Form Converter (BNFC)}
\label{sec:BNFC}

The Backus-Naur Form Converter (BNFC) is a tool for generating lexer and parser
components. It takes as an input a Labelled BNF grammar \cite{LBNFReport},
in the form of a {\ttfamily .cf} file, and produces a set of source code files
which, when used together, can parse the source language described in the
grammar. Given some input language specification {\ttfamily source\_lang.cf},
a few important output files are produced. A description is provided for each
file:
\begin{itemize}
    \item {\ttfamily AbsSourceLang.hs} - a Haskell module containing the Abstract
        Data Types for the nodes of the Abstract Syntax Tree built by the
        parser
    \item {\ttfamily LexSourceLang.x} - an Alex file containing a description
        of the lexer for the source language
    \item {\ttfamily ParSourceLang.y} - a Happy file containing a description
        of the parser for the source language
    \item {\ttfamily ErrM.hs} - a Haskell module describing a Monad for handling
        errors when parsing the source language (Monads will be discussed more
        lexer and parser respectively, and compiles and links TestSourceLang.hs
        to create an executable
\end{itemize}

The University of York Computer Science department makes use of the BNFC tool
in the Implementation of Programming Languages (IMPL) course, where students
have the opportunity to investigate various components of the compiler pipeline.
Because BNFC allows students to rapidly prototype language ideas, and the files
generated can be utilised to produce fully working compilers, the tool is great
for learning and for small projects.

While writing the lexer and parser components for the type system markup language
myself might result in a more efficient and complete product, I will instead be
making use of the BNFC tool to create the front end components. This is not only
because of the pros of the tool discussed above, but also due to the timescale
of the project. Attempting to construct the front-end components alongside
completing my other 3rd year modules simply wouldn't be viable. Using the BNFC
tool also provides a more complicated LBNF example for IMPL students to
investigate in future years of study.

\section{Attribute Grammars \& Syntax Directed Translation}
We described type checking in the previous chapter, but how do we
integrate it into the compiler pipeline? One method, conceived by Donald Knuth
is the Attribute Grammar \cite{KnuthGrammars}. An attribute grammar is a
context-free grammar which has been extended to include additional semantic
rules, tied to each non-terminal symbol. These additional rules are known as
attributes. Attribute grammars are sometimes referred to as 
\textit{syntax-directed definitions} which are used to perform 
\textit{syntax-directed translation} \cite{DragonBook}. This is because the 
abstract syntax tree is the designated data structure which is carried
through many of the compilation phases.

\subsection{Formal Definition}
\label{sec:FormalDefinition}
Recall the definition of a context-free grammar, as specified by John C. Martin
\cite{Martin}:
\begin{definition}[Context-Free Grammar] \label{def:CFG}
    A Context Free Grammar (CFG) is a 4-tuple $G = \langle V, \Sigma, S, 
    P\rangle$, where $V$ and $\,\Sigma$ are finite sets, $S \in V$, and $P$ is a
    finite set of formulas of the form $A \rightarrow \alpha$, where $A \in V$
    and $\alpha \in (V \cup \Sigma)^\ast$.

    Elements of $\Sigma$ are terminal symbols, elements of $V$ are non-terminal
    symbols, $S$ is the start symbol, and elements of $P$ are production rules.
\end{definition}
By extending Definition \ref{def:CFG}, we can achieve the formal definition of 
an attribute grammar originally specified by Knuth \cite{KnuthGrammars}, but 
presented in a similar tuple style, as defined by Waite and Goos 
\cite{WaiteBook}:
\begin{definition}[Attribute Grammar] \label{def:AG}
    An Attribute Grammar is an 4-tuple $AG = \langle G, A, R, B\rangle$, where
    $G$ is a Context-Free Grammar, $A$ is a finite set of attributes, $R$ 
    is a finite set of semantic rules defined using the attributes in $A$ and
    $B$ is a set of predicates.
\end{definition}
The set $A$ can be deconstructed into two disjoint sets $S$ and $I$. These
represent two types of attributes, \textit{synthesised attributes} and 
\textit{inherited attributes} respectively. As defined by Louden, an attribute
is \textit{synthesised} if \textit{``all of its dependencies point from child 
    to parent in the parse tree"} \cite[\S6.2.2, p.~277]{Louden}. An attribute
is \textit{inherited} if all of its dependencies point either from 
parent to child in the parse tree, or sibling to sibling 
\cite[\S6.2.2, p.~278]{Louden}. Each grammar symbol $X \in V \cup \Sigma$ is 
associated with a set of synthesised attributes and a set of inherited 
attributes. Each production rule $p \in P$ is associated with a set of 
semantic rules.

\subsection{Example: Assigning Types to Declared Variables}
Louden provides a good simple example of an attribute grammar for variable
declarations in a C-like language \cite{Louden}, shown in Figure 
\ref{fig:AttributeGrammar}. Some symbols have been renamed from the original
example to improve readability, these are $Type$ to $DataType$, and $dtype$ to
$type$.

\begin{figure}
    \begin{minipage}[b]{.5\textwidth}
        \begin{align*}
            Decl &\rightarrow DataType \; VarList \\
            DataType &\rightarrow int \\
            DataType &\rightarrow float \\
            VarList_1 &\rightarrow id \, "," \, VarList_2\\
            VarList &\rightarrow id 
        \end{align*}
        \subcaption{Production Rules}
        \label{fig:ProductionRules}
    \end{minipage}
    \begin{minipage}[b]{.5\textwidth}
        \begin{align*}
            VarList.type &= DataType.type \\
            DataType.type &= integer \\
            DataType.type &= real \\
            id.type &= VarList_{1}.type \\
            VarList_{2}.type &= VarList_{1}.type \\
            id.type &= VarList.type
        \end{align*}
        \subcaption{Attribute Definitions}
        \label{fig:AttributeDefinitions}
    \end{minipage}
    \caption{An attribute grammar for variable declarations}
    \label{fig:AttributeGrammar}
\end{figure}

Suppose we were to parse the statement {\ttfamily int a, b} using this grammar. 
In order to do this, we would need to compute the dependency graph associated 
with each set of semantic rules mapped to each grammar production. 
Using these graphs, we can construct the dependency graph for our input string 
by uniting the associated dependency graph for each rule used in the derivation 
of our parse tree \cite{Louden}. This dependency graph will tell us what order 
we should annotate the parse tree nodes . The resulting annotated parse tree 
is shown in Figure \ref{fig:AGAnnotatedParseTree}, with the dependency graph 
overlayed with dashed lines.

\begin{figure}
    \Tree [.Decl\\(type=integer) [.DataType\\(type=integer) int ] 
    [.VarList\\(type=integer) id(a)\\(type=integer) "," 
    [.VarList\\(type=integer) id(b)\\(type=integer) ] ] 
    ]
    \caption{The annotated parse tree for the expression {\ttfamily int a, b},
        with its associated dependency graph}
    \label{fig:AGAnnotatedParseTree}
\end{figure}

\subsection{Implementation \& Critique}
\label{sec:ImplAndCritique}
By encoding our type rules as a series of attributes and semantic rules, we can
perform our type checking by scanning up and down the tree and applying the
relevant rules to each node. This provides an implementation for the type
checking process described in Section \ref{sec:Chap1TypeChecking}. The problem 
of generating a type checker reduces to generating an attribute grammar $AG$, 
given some Context-Free Grammar $G$ representing the source language 
specification, with each type rule being encoded in the form of semantic rules 
tied to each production. 

If both synthesised and inherited attributes are utilised in an attribute
grammar, then it must be checked for circularity. This is to make sure
that all defined semantic rules will evaluate to a value, and not endlessly
loop forever. An algorithm for this was devised by Knuth \cite{KnuthGrammars}
\cite{KnuthCorrection}, and later improved upon by Jazayeri, Ogden and Rounds.
Unfortunately the time complexity of the algorithm was determined to be 
$O(2^{dn})$, where $d$ is a positive constant, and $n$ is the 
description size of the grammar \cite{OgdenAGComplexity}. To make matters even 
worse, Jazayeri et al. determined that the minimum number of steps required for 
any circularity algorithm is $O(2^{\sfrac{cn}{\log n}})$, where $c$ is a 
positive constant. The difficulty of checking for circularity is thus 
a drawback for using attribute grammars to specify static semantics.

Another drawback is linked to the formal definition of the Attribute Grammar
itself. It is known that the formal version of an attribute grammar is 
considered difficult to implement into compiler pipelines. In Waite's 
paper \textit{Use of Attribute Grammars in Compiler Construction},he states 
that \textit{``Experience has shown that [the 4-tuple] definition of 
    an attribute grammar must be augmented if it is to be of practical use in 
    compiler construction"} \cite{WaitePaper}. The paper goes on to explain 
the complexities of accessing attribute information beyond the local parent and 
child relationships established in the semantic rules associated with each
production. With the current attribute grammar definition, the only way to pass 
this information from two distant nodes in the parse tree would be to pipe the 
information through the tree, increasing the complexity of the grammar greatly.

\section{Applying Category Theory To Type Checking}
\label{sec:CategoryTheory}
Using a functional language like Haskell provides us with the capability to
create elegant abstractions with which we can perform powerful computations
with. By borrowing some ideas from Category Theory, we can enhance our Abstract
Syntax data structures with desirable properties to make type checker generation
easier.

\subsection{Functors}
The first abstraction we will borrow from Category Theory is the Functor. The
use of higher order functions is becoming ever more prevalent in modern
programming languages, especially functions such as \lstinline{map},
\lstinline{filter} and \lstinline{foldr}. The \lstinline{map} function in
particular is incredibly powerful, as it can take a function and a list as
input, and repeatedly apply that function to every element in the list returning
a new list as a result. The following Haskell code provides an implementation
of \lstinline{map}:
\lstset{language=Haskell,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{Orange},
    identifierstyle=\color{MidnightBlue},
    stringstyle=\color{OliveGreen},
    showstringspaces=false,
    frame=single}
\lstinputlisting{code-examples/map.hs}
From looking at the function signature, it can be seen that map only works on
lists. But what if we wanted to apply map to a data structure other than list?
The Functor allows us to do this. A data structure is considered a Functor if
it provides an implementation of the \lstinline{fmap} function, with the
following type signature:
\lstinputlisting{code-examples/fmap.hs}
Here, \lstinline{f} is our data structure. The \lstinline{fmap} function
generalises the map function to some data structure \lstinline{f}. An example
of this concept being used specifically in this project could be...

\textit{I'm working on a suitable example for this section...}

\subsection{Monads}
We can extend the concept of the Functor further to make use of another
abstraction known as a Monad. A Monad is a data structure which implements the
following two operations, described here in Haskell:
\lstinputlisting{code-examples/monad.hs}
Monads provide us with a way of seperating pure and impure computation which
may have side-effects. A very useful application of this is in error handling
and performing operations which may or may not fail. If we find when type
checking that our program fails, we should attempt to diagnose the problem and
report back to the user. A data structure for this very purpose is generated by
the BNFC tool and stored in the {\ttfamily ErrM.hs} file mentioned in Section
\ref{sec:BNFC}, shown below:
\lstinputlisting{code-examples/err.hs}
If the computation is successful then we can return some value of type
{\ttfamily a}, otherwise we return a string which explains the error. The
{\ttfamily >>=} operator provides us with to chain together computations using
the {\ttfamily Err} data type. If at any point the computation produces an
error, we can simply propagate the error message to the end of the computation
chain. Consider the following type checking code:
%\lstinputlisting{code-examples/monad_type_check.hs}

\textit{Working on a do-notation example demonstrating the Err data structure...}

\subsection{Catamorphisms}

\cite{PierceCategory}
\cite{MilewskiCTFP}
\cite{HuttonHaskell}
\cite{Catamorphisms}

\section{Advanced Type Rules \& Context}
With attribute grammars annotating the program abstract syntax tree with type
information using attributes, an alternative approach involves processing the
abstract syntax tree using pattern matching functions in a functional programming
language like Haskell. These functions would implement type rules like those
described in Section \ref{sec:Chap1TypeSystems} and \ref{sec:Chap1TypeChecking}.
To begin thinking about how we might construct these functions, we should first
consider what form they might take, and whether the form is predictable enough
to generate.

\subsection{Representing Simple Type Rules in a Programming Language}
Consider the following type rule:
\begin{displaymath}
    \prftree[l]{||}{\Gamma \vdash a:\mathbb{B}}{\Gamma \vdash b:\mathbb{B}}{
    \Gamma \vdash a \, || \, b:\mathbb{B}}
\end{displaymath}
Rules like the one above are easy to translate into
Haskell and seem to have a predictable form which can be generated. Consider
the following Haskell source code:
\lstinputlisting{code-examples/and_type_rule.hs}

The consequents of the rule appear as arguments to the check function on the
left hand side, and the antecedents appear as the function body on the right
hand side. The TrueLiteral and FalseLiteral rules return \lstinline{Ok True}
because they are axioms and have no antecedents. Type rules which don't mutate
the program context should be easy to generate because they don't require us
to lookup the types of any variables. The complexity of our type rule
implementations will begin to develop when we need to start considering the
program context when type checking.

\subsection{Context, Environment \& Scoping}
If we are type checking an expression that is incredibly simple and only uses
literal values such as True, False and 42, then we won't need to consult the
program context. Unfortunately, programming languages which only make use of
literal types don't gain much traction in the real world because they aren't
very useful. Modern languages make use of more powerful language concepts such
as variables and functions, concepts which require us to keep track of
declarations and definitions in order to type check our programs correctly.
This is precisely what the context structure is for, to store variable-type
pairs or function type-pairs.

The design and behaviour of the context data structure used for storing these
bindings is largely dictated by the scoping rules of the programming language.
If the language decided to make use of static scoping, then the contexts
structures may be pushed and popped from a context stack as new blocks are
entered and exited. If dynamic scoping were used instead, then a different
implementation would be required. The designer of the type system would need
to describe the behaviour of the context data structure in the type system
language to make the type checker generator as flexible as possible.

Ranta \cite{Ranta} proposes a model for contexts appropriate for a statically
scoped langauge in the form of the following Haskell source code:
\lstinputlisting{code-examples/context.hs}
The code describes the context as a pairing of a hash map for mapping function
names to their corresponding input and output types, and a list of hash maps
mapping variable names to types. The function declarations remain global and can
be seen as they are acquired by the type checker. The contexts can be pushed
and popped from the context stack as they are encountered when traversing the
parse tree.

\subsection{More Advanced Type Rules}
Now consider the following type rule for type checking a variable, and its
corresponding code:
\begin{displaymath}
    \prftree[l,r]{[if $v:T$ in $\Gamma$]}{VarT}{\Gamma \vdash v:T}
\end{displaymath}

\lstinputlisting{code-examples/var_type_rule.hs}
This rule seems more complicated to express in a programming language, as it
requires us to check whether we have a variable binding in our context which
matches the variable we are checking. Looking up a variable would be considered
a side-condition in a type rules, and it seems that any rules which have
side-conditions will be harder to generate.

\chapter{Problem Analysis}

\section{Focusing the Project}
In order to give this project focus, a hypothesis and research question need to
be formulated, along with expected observations. This allows the 
success of the project to be measured by comparing the initial predictions with 
the results, and discussing the implications. Using the project brief and the
knowledge gathered from the literature review, the following hypothesis has been
formulated:
\begin{hypothesis}
    A static semantics for a given source language is much harder to generate
    than a lexer or a parser, and will require more complex generation methods.
\end{hypothesis}
The reasoning behind this hypothesis is based on the conclusions reached in
Section \ref{sec:ImplAndCritique} regarding the complexities of implementing 
attribute grammars, as well as the fact that attribute grammars are 
themselves context-free grammars with added complexity, as shown in Definition 
\ref{def:AG} in Section \ref{sec:FormalDefinition}.
To help define a scope of investigation for this hypothesis, a research question 
is derived from it. By combining the research question with the constraints of the
project brief, we can clearly establish the project's scope and minimise any 
ambiguity regarding the aims of the project and what we are investigating.
\begin{researchquestion}
    Is it possible to construct a generic type checker generator, which, when 
    given a source language specification and a set of type rules, will produce 
    a type checker for the source language?
\end{researchquestion}


\chapter{Solution Design}

\section{Requirements}
\label{sec:Requirements}
As declared by the brief, the type system language and generator have the
following requirements:
\begin{enumerate}
    \item The type checker generator should be compatible with source languages
        specified for use with the BNFC tool, in the form of a {\ttfamily .cf}
        file.
    \item A type checker should be able to be generated for a simple procedural 
        language containing various operators and a procedure mechanism.
    \item If time permits, additional support should be added for type checking
        more advanced language features such as fixed arrays, records and 
        pointers.
    \item The type system description language needs to be readable, whilst
        also maintaining expressibility.
    \item The type system description language needs to be generic and not
        specific to a particular source language. This is required to keep
        the generator useful and applicable to a large number of source languages.
\end{enumerate}

\section{Initial Assumptions}
Before we can begin designing the language or the generator, we must first state 
some initial assumptions being made about the state of the inputs, and the 
environment in which the type checker generation will occur. We will assume that:
\begin{itemize}
    \item The source language specification is a Labelled BNF Grammar, stored 
        in a {\ttfamily .cf} file, within the same directory as the type
        checker generator. This is to satisfy the first requirement.
    \item The BNFC tool has successfully generated a working lexer/parser unit
        for the input source language, with the modules generated from that
        process available in the current directory. This is to provide the
        generator with a predictable set of data structures and parser functions 
        to work with.
\end{itemize}

\section{Relating Source Languages to the Generation Pipeline}
In the previous sections, we have discussed methods of type checking and where
the type checker sits in the source language compiler pipeline. To better 
understand where the generator sits in relation to this pipeline, a diagram 
relating the two is shown in Figure \ref{fig:SoftwareComponents}. The parts of 
the diagram highlighted in red are the software components that will be 
constructed during the project. The type checker generation pipeline runs 
orthogonally to the type checker pipeline. 

\begin{figure}[!ht]
    \begin{tikzpicture}
        \node (SLSC) [label] {\footnotesize Source Language Source Code};
        \node (TLS) [label, above of=SLSC, color=red, yshift=2.25cm] {\footnotesize Type System Language Specification};
        \node (SLL/P) [archbox, right of=SLSC, xshift=3cm] {\footnotesize Source Language Lexer/Parser};
        \node (SLTC) [archbox, right of=SLL/P, xshift=3cm] {\footnotesize Source Language Type Checker};
        \node (TCG) [archbox, above of=SLTC, yshift=0.75cm, color=red] {\footnotesize Type Checker Generator};
        \node (BNFC) [archbox, above of=SLL/P, yshift=2.25cm] {\footnotesize BNFC};
        \node (TCGL/P) [archbox, above of=TCG, yshift=0.5cm, xshift=1cm, color=red] {\footnotesize Type Rule Lexer/Parser};
        \node (CBE) [dashedarchbox, right of=SLTC, xshift=3cm] {\footnotesize Compiler Back-End};
        \node (TR) [label, above of=TCGL/P, yshift=0.5cm] {\footnotesize Type Rule Specification};
        \node (SLS) [label, above of=BNFC, yshift=0.5cm] {\footnotesize Source Language Specification};

        \draw [arrow] (SLS) -- (BNFC);
        \draw [arrow] (TR) -- (TCGL/P);
        \draw [arrow] (SLSC) -- (SLL/P);
        \draw [arrow] (TLS) -- (BNFC);
        \draw [arrow] (BNFC) -- node [anchor=east] {\scriptsize generates}(SLL/P);
        \draw [arrow] (BNFC) -- node [anchor=south] {\scriptsize generates}(TCGL/P);
        \draw [arrow] (SLL/P) -- (SLTC);
        \draw [arrow] (TCG) -- node [anchor=west] {\scriptsize generates} (SLTC);
        \draw [arrow] (TCGL/P) -- (TCG);
        \draw [arrow] (SLTC) -- (CBE);
        \draw [arrow] (SLS) .. controls +(up:4cm) and +(right:12cm) .. (TCG);
    \end{tikzpicture}

    \caption{A diagram relating the various components of the source language 
        compiler pipeline with the type checker generation pipeline}
    \label{fig:SoftwareComponents}
\end{figure}

\section{Language Features}

\subsection{Maximising Language Readability}
To keep the language readable and satisfying requirement 4, it should relate to 
the type rules it is modelling in the most direct way possible. Therefore, 
basing the syntax of the description language on the mathematical notation used 
to describe type rules seems like a good place to start.

\begin{figure}[!ht]
    \begin{minipage}[b]{.55\textwidth}
        \begin{displaymath}
    \prftree[l,r]{\scriptsize [Side-Conditions]}{\scriptsize RuleName}{
        J_1 \dots J_n}{C}
        \end{displaymath}
        \subcaption{Type rule}
        \label{fig:TypicalTypeRule}
    \end{minipage}
    \begin{minipage}[b]{.45\textwidth}
    \lstset{basicstyle=\footnotesize\ttfamily, frame=single,
        language=,}
    \lstinputlisting{code-examples/rule_syntax.txt}
        \subcaption{Syntax inspired by type rule notation}
        \label{fig:ProposedSyntax}
    \end{minipage}
    \caption{The mathematical notation used for describing type rules, contrasted
        with a proposed syntax for type rules in the description language}
    \label{fig:RuleLanguageContrast}
\end{figure}

\subsection{Maintaining Generality}
One of the caveats we need to consider is how to keep the language as general as
possible, in order to satisfy requirement 5. Suppose we had the following type rule expressed in our description
language:
\lstinputlisting{code-examples/specific_rule.txt}
With this syntax, an implicit assumption is made, that is, the description
language has a full understanding of how to syntactically form any expression, 
statement or declaration in the source language. With the example source language,
addition is written using an infix style. This may not be the case with all 
languages however. If we wanted to create a type checker for a LISP-like language,
addition would be expressed in a prefix style as {\ttfamily (+ a b)}, with the
type rule now being:
\lstinputlisting{code-examples/lisp_like_rule.txt}
We therefore cannot realistically maintain this assumption as it would require
our language to have a working knowledge of how to form the language constructs
of every possible source language that could feasibly be passed to the generator 
as an input. Practically speaking, this is impossible. If we wanted to maintain
this assumption, the only way we could do so would be modify the specification
for the lexer and parser of the description language every time we worked with
a new source language. Doing this however is not only inconvenient for the user,
it also results in a loss in generality of the language.

One way to remove the dependency on the source language from the description
language is to encode the various language constructs of the source language as 
strings when describing them. These strings can be sent to the source language 
parser by the generator to produce the corresponding abstract syntax trees, 
without the generator having to worry about the syntactic forms of the various 
language constructs in the source language. This results in a rule syntax 
resembling the following:
\lstinputlisting{code-examples/string_rule.txt}
Unfortunately there are pitfalls with this syntax also. The problem here is that 
in order to successfully describe the type rule for a language construct, the
strings being used to express the components of the rule must be 
successfully parsable. If a string were to fail the parsing stage, the type 
rule cannot be generated. As there is no way of expressing a general term of
the source language in the source language itself, this syntax would only
be able to create type rules for specific values e.g.
{\ttfamily 1+2}. This is not expressive and can't cover enough scope of the 
source language to be useful.

The only feasible way to maintain generality and construct a description
language that is not specific to one specific source language is with the
help of the user. The string idea can still be used, but instead of encoding 
the syntax of the source language, the user provides the BNF label used in their
language specification for the construct they are describing, along with the
construct parameters. For example, if the add operation {\ttfamily a+b} has
the Labelled BNF production
\lstinputlisting{code-examples/lbnf_add.txt}
then the type rule for this expression would be represented as:
\lstinputlisting{code-examples/AS_rule.txt}
where {\ttfamily "a"} and {\ttfamily "b"} in the type rule correspond to the 
{\ttfamily Exp1} and {\ttfamily Exp} terms respectively in the Labelled BNF 
production.

\subsection{Supporting Various Types}
To meet requirement 2, the description language needs to be able to support the
fundamental types of a simple procedural language. The language should also be
designed in such a way that it is easy to extend to support new types. To achieve
this support, the language should have its own set of types independent of the
source language, rather than trying to reason about the source language types
using the types themselves. This is because if we are trying to reason generally
about types with varying size, the source language type annotations may not
be sufficient to achieve this. For example, a record type could have up to $n$
members stored within it, but the source language record syntax may require the
user to explicitly enumerate the members of the record, fixing the size to a 
specific value. This would result in the type rule only applying to records of
the specified size, rather than all records.

The types required to satisfy requirement 2 can be expressed using the following
description language expressions:
\begin{itemize}
    \item {\ttfamily Int}: 32 bit integer values
    \item {\ttfamily Long}: 64 bit integer values
    \item {\ttfamily Float}: 32 bit floating point values
    \item {\ttfamily Double}: 64 bit floating point values
    \item {\ttfamily Char}: Single characters encoded as 8 bit values
    \item {\ttfamily String}: Sequences of characters
    \item {\ttfamily Bool}: Boolean values
    \item {\ttfamily Stm}: Statement type (synonymous with the {\ttfamily Unit}
        type in Haskell)
\end{itemize}
Requirement 3 can be satisfied with the addition of the following expressions
the description language:
\begin{itemize}
    \item {\ttfamily FArray[T, n]}: Fixed size array type where {\ttfamily T} is 
        the type of values contained in the array, and {\ttfamily n} is the 
        array length.
    \item {\ttfamily VArray[T]}: Variable size array type where {\ttfamily T} is
        the type of values contained in the array.
    \item {\ttfamily Pointer[T]}: Pointer to value of type T.
\end{itemize}


\subsection{Adding a Context Description System}

\subsection{Adding Support for Different Side-Conditions}

\chapter{Implementation}

\chapter{Results and Evaluation}

\printbibliography
\end{document}
