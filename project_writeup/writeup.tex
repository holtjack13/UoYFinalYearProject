% Author: Jack Holt

% Final Year Project writeup .tex file

\documentclass{UoYCSproject}

% Package declarations
\usepackage{prftree} % Proof Trees and Natural Deduction Rules
\usepackage{listings} % Typesetting Source Code Examples

\usepackage{qtree} % Parse Trees
% Sets courier font on parse trees
\newcommand{\qlabelhook}{\ttfamily}
\newcommand{\qleafhook}{\ttfamily}

\usepackage[dvipsnames]{xcolor} % More colors

% Flow Diagrams
\usepackage{tikz}
\usetikzlibrary{arrows, shapes.geometric}

\usepackage{amsmath} %embedding normal text in math block

\usepackage{subcaption} % Used for multi-figures

% Nice code font
\usepackage[T1]{fontenc}
\usepackage{sourcecodepro}

% Bibliography file
\addbibresource{project.bib}

% Front Matter:
\title{Generation of Type Checking Code}
\author{Jack Holt}
\date{22nd March 2019}
\supervisor{Jeremy L. Jacob}
\BEng % Degree Type

\dedication{To Mr Rowley, who revealed the incredible depth and power of
    mathematics to my unsuspecting mind for the first time.}
\acknowledgements{I would like to thank my supervisor, Jeremy Jacob, for his
    continued assistance and informative feedback over the course of this
    project. I would also like to thank my family for their support when I've
    needed it most, and my friends for making the journey to this point a
    memorable one.}

% Project Content
\begin{document}

% Make flowchart boxes used in figures
\tikzstyle{flowbox} = [rectangle, rounded corners, text centered, draw=black]
\tikzstyle{archbox} = [rectangle, text centered, draw=black,
text width=0.25*\columnwidth, minimum height=1cm]
\tikzstyle{dashedarchbox} = [rectangle, text centered, draw=black, dashed,
text width=0.25*\columnwidth, minimum height=1cm]
\tikzstyle{label} = [text centered, text width=0.25*\columnwidth]
\tikzstyle{arrow} = [thick, ->, >=stealth]

% Instantiate front matter and other beginning pages
\maketitle
\listoffigures
\pagenumbering{roman}

% Summary
\begin{summary}
\end{summary}

\chapter{Introduction}

\section{Project Outline}

% Outline of Hypothesis and Research Question
\chapter{Background}

\section{The Role of Types in Programming Languages}
Many modern programming languages today support and encourage the use of data
types as a way of organising our data and creating abstractions with which
we can perform computation. Whether or not data types are implemented in a
programming language can be considered a design trade-off by the language
designer.

Not implementing data types could make the implementation of a language easier
or more difficult relative to typed languages, depending on the priority of
language safety. In order for a untyped language to be considered safe, as
defined by Cardelli \cite[p.~3]{CSHandbook}, run-time checks would need to be added
to check for \textit{untrapped errors}, such as incorrect memory jumps and
reading memory past the bounds of an array. These checks could result in
decreased run-time performance of programs. LISP, developed by John McCarthy in
1958, is considered to be an untyped safe language, whilst most assembly
languages are classified as untyped and unsafe. Typed languages can usually
eliminate the need for many run-time checks by performing them at compile-time.

In some cases untyped languages provide a higher level of expressibility for
the programmer, as there are fewer type restrictions on each operation.
LISP is known for its expressive power due to its ability to treat programs and
data as one and the same thing. A consequence of this is the ease of which
powerful programming techniques such as meta-programming can be utilised to
produce domain specific languages designed for solving a distinct class of
problems \cite[p.~5]{SICP}. Figure \ref{fig:SimpleTypedLanguages} contains a
simple LISP example showcasing its syntax.

\begin{figure}
    \lstset{frame=single,
        language=Lisp,
        numbers=left,
        basicstyle=\footnotesize\ttfamily,
        keywordstyle=\color{Orange},
        identifierstyle=\color{MidnightBlue},
        stringstyle=\color{OliveGreen},
    }
    \lstinputlisting{code-examples/programs_and_data.rkt}
    \caption{A LISP example}
    \label{fig:SimpleTypedLanguages}
\end{figure}

Introducing static typing into a programming language provides its own benefits
to both the programmer and the language compiler/interpreter. By analysing the
types of each computational expression in a given program at compile-time,
the language can assist the programmer in finding type
inconsistencies which would result in silent erroneous calculations. This type
analysis can also be used by the compiler to allocate the optimal amount of
space required to store the information used by the program.

Consider the C++ program in Figure \ref{fig:C++TypeError}.
There are no syntax errors, but it is still incorrect due to the
``x+y" expression in Line~9. An attempt is made to add an integer to a string,
an operation whose meaning is ambiguous without further specification. This
error is caught at compile-time thanks to the strong type system of C++.

Because of the prevalence of strong type systems in modern programming
languages, it could be argued that modern programmers have accepted they would
rather have better checking capabilities with less expressibility, than more
expressibility with worse checking capabilities. Hence the study of type systems
and type checking seems worthwhile and in a modern language designer's best
interests.

% C++ Code Example
\begin{figure}
\lstset{language=C++,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{Orange},
    identifierstyle=\color{MidnightBlue},
    stringstyle=\color{OliveGreen},
    morecomment=[l][\color{Plum}]{\#},
    morekeywords={string},
    numbers=left,
    frame=single}

\lstinputlisting{code-examples/type_inconsistency.cpp}
\caption{Example C++ program with a type error.}
\label{fig:C++TypeError}
\end{figure}

\section{Static Semantics}
% Talk about how static semantics are usually derived from
% dynamic semantics to prove that the relationship between the two is preserved.
% For this project, not our responsibility to do this, we are trying to create
% a system which generates a type checker from specified type rules. The
% derivation of these rules is not performed by the program.
The concept of type checking falls under the study of programming language
semantics, the study of program meaning (as opposed to language syntax, which is
the study of program form). It is vital for the semantic rules of a programming
language to assign precisely one meaning to each program expressed in that
language. Failure to do so would result in the potential creation of an
ambiguous program which has multiple meanings, and may behave differently every
time it is compiled or interpreted \cite[p.~114]{Sebesta}.

Semantics can be further broken down into two kinds, static semantics and
dynamic semantics. Static semantics concerns itself with various kinds
of compile-time checks such as type checking and scope analysis. Mosses
describes static semantics as the \textit{``checking for well-formedness"} of
programs before they are compiled and executed \cite{Mosses}. Dynamic
semantics is focused more on the program meaning described in the previous
paragraph, and requires the program to be executed in order for that meaning to
be determined.

As this project is focused on type checking, dynamic semantics
isn't explored here. However a few papers have been written
proposing various algorithms and methods for deriving type rules and type
systems from dynamic semantic rules \cite{NeilJones} \cite{JohnHannan}. This could
be an interesting avenue to explore in future projects relating to type checkers.

\section{Type Systems \& Type Rules}
\label{sec:Chap1TypeSystems}

% Introducing the two major views on type systems
A type system, as described by Sebesta, \textit{``is a set of types and the rules that
    govern their use in programs"} \cite[\S6, p.~309]{Sebesta}.
There are two main views on what a type system encompasses, pioneered by two
major computer scientists, Alonzo Church and Haskell B. Curry.
Church took what is known as the ``prescriptive" view of type systems, which is
the belief that \textit{``types are predefined conditions to ensure meaningfulness"}
and \textit{``for a program to even have a meaning, it must be well-typed"}
\cite{NeilJones}. Curry adopted an opposing ``descriptive" view of type systems
and believed that \textit{``any program can be executed"} and \textit{``a type
    is a way of classifying or describing the values that a program manipulates"}
\cite{NeilJones}.

% Compare the two views, and conclude that we will be following the descriptive
% view of types.
The type systems which will be generated with the generation tool will be
descriptive. We will not be considering typeless programs or expressions which
despite not having a type, still perform interesting computation (for example
the Y-Combinator in the simply typed lambda calculus)\cite{NeilJones} \cite[p.~28]{SimonPeytonJones}
\cite[p.~155]{SimonPeytonJones}.

We can express our type rules as natural deduction rules using the following
syntax:
\begin{displaymath}
    \prftree[l,r]{\scriptsize [Side-Conditions]}{\scriptsize RuleName}{J_1 \dots J_n}{C}
\end{displaymath}
where $J_k$ for $k=1, \dots ,n$ and $C$ are judgements. Judgements are logical 
statements of the form:
\begin{displaymath}
    \Gamma \vdash \mathbb{A}
\end{displaymath}
Here, $\mathbb{A}$ represents an assertion which is made about some object and
its type. $\Gamma$ represents the environment (or context as it's sometimes
referred to) in which the assertion in evaluated. This notation is used by both
Cardelli \cite[p.~8]{CSHandbook} and by Ranta \cite{Ranta} to describe judgements.

The decision to express type rules using this style of notation stems from the 
desire to maintain a common language of type rules for readers of Ranta's key
text \textit{``Implementing Programming Languages"} \cite{Ranta}, which is a key 
text for learning the BNFC tool \cite{BNFC}. The BNFC tool is used throughout
this project, with a detailed breakdown of it taking place in Section 
\ref{sec:BNFC}. 

An example of a type rule in a simple language for evaluating boolean 
expressions is as follows:
\begin{displaymath}
    \prftree[l]{\scriptsize \&\&}{\Gamma \vdash a:Bool}{\Gamma \vdash b:Bool}{\Gamma \vdash a \, \&\& \, b: Bool}
\end{displaymath}
This rule reads as ``given any context $\Gamma$, the expression $a \, \&\& \, b$
is of type $Bool$ only if the expression $a$ is of type $Bool$ and the expression $b$
is of type $Bool$.

\section{Type Checking}
\label{sec:Chap1TypeChecking}
The type checker is the next component in the compiler pipeline after the
lexing/parsing unit. As an input, the type checker receives an Abstract Syntax
Tree from the parser. The tree is a representation of the syntactical structure
of the program. The type checker steps through this Abstract Syntax Tree, applying
type rules like those discussed in the previous section, to each node. If we
can construct a tree of type rule deductions in this fashion, then we have a
proof that all expressions and statements in that program are type consistent.
If however, a tree cannot be constructed, then the program is improper and
can be rejected.

Consider Figure \ref{fig:ParseAndProofTree}, which uses the
type rule specified in the previous section:
To stop the proof tree from becoming too wide, I've abbreviated $True$ to $\top$,
$False$ to $\bot$ and the type $Bool$ to $\mathbb{B}$.
% Parse and Proof Tree Example
\begin{figure}
    \begin{tikzpicture}
        \node (Code) [flowbox] {\ttfamily True \&\& False \&\& True};
        \node (ParseTree) [flowbox, below of=Code, yshift=-2cm] {\Tree [.EAnd [.EBool True ] [.EAnd [.EBool False ] [.EBool True ] ] ]};
        \node (ProofTree) [flowbox, below of=ParseTree, yshift=-3.2cm, minimum height=2.8cm, minimum width=12.5cm] {
            \prftree[l]{\scriptsize \&\&}{
                \prftree[l]{\scriptsize trueLiteral}{\Gamma \vdash \top:\mathbb{B}}}{
                \prftree[l]{\scriptsize \&\&}{
                    \prftree[l]{\scriptsize falseLiteral}{\Gamma \vdash \bot:\mathbb{B}}}{
                    \prftree[l]{\scriptsize trueLiteral}{\Gamma \vdash \top:\mathbb{B}}}{
                    \Gamma \vdash \bot \, \&\& \, \top: \mathbb{B}}}{
                \Gamma \vdash \top \, \&\& \, (\bot \, \&\& \, \top): \mathbb{B}}
            };
            \draw [arrow] (Code) -- node [anchor=west] {\scriptsize when parsed becomes} (ParseTree);
            \draw [arrow] (ParseTree) -- node [anchor=west] {\scriptsize is proven to be well-typed by} (ProofTree);
        \end{tikzpicture}
    \caption{A abstract syntax tree for a simple boolean expression, and its corresponding proof tree}
    \label{fig:ParseAndProofTree}
\end{figure}
Notice how the proof tree mirrors the structure of the Abstract Syntax Tree.
Each type rule application happens at a node in the parse tree.

\section{Generating Type Checkers vs. Generating Lexers \& Parsers}
The mathematical tools of choice for a compiler designer looking to design
a Lexer or Parser for their language are the Finite State Automaton and the
Pushdown Automaton. These automata are well understood by the computer science
community, so much so that computer programs exist which can take descriptions
of these machines in the form of Regular Expressions and Context-Free Grammars
and produce the machines they describe as an executable program. These programs
are incredibly useful for compiler designers, and speed up the design and
implementation phases of compiler construction. Examples of such Lexer and Parser
generators are Alex and Happy, which are employed by the BNFC tool.

Whilst the methods of expressing Lexers and Parsers seems to be agreed on by
many, the best way of describing a Type System or a Static Semantics is still
debated \cite{NeilJones}.

% Exploration of currently available tools and key texts and literature
\chapter{Literature Review}

\section{Backus-Naur Form Converter (BNFC)}
\label{sec:BNFC}

The Backus-Naur Form Converter (BNFC) is a tool for generating lexer and parser
compiler units. It takes as an input a Labelled BNF grammar \cite{LBNFReport},
in the form of a {\ttfamily .cf} file, and produces a set of source code files
which, when used together, can parse the source language described in the
grammar. Given some input language specification {\ttfamily source\_lang.cf},
a few important output files are produced. A description is provided for each
file:
\begin{itemize}
    \item {\ttfamily AbsSourceLang.hs} - a Haskell module containing the Abstract
        Data Types for the nodes of the Abstract Syntax Tree built by the
        parser
    \item {\ttfamily LexSourceLang.x} - an Alex file containing a description
        of the lexer for the source language
    \item {\ttfamily ParSourceLang.y} - a Happy file containing a description
        of the parser for the source language
    \item {\ttfamily ErrM.hs} - a Haskell module describing a Monad for handling
        errors when parsing the source language (Monads will be discussed more
        lexer and parser respectively, and compiles and links TestSourceLang.hs
        to create an executable
\end{itemize}

The University of York Computer Science department makes use of the BNFC tool
in the Implementation of Programming Languages (IMPL) course, where students
have the opportunity to investigate various components of the compiler pipeline.
Because BNFC allows students to rapidly prototype language ideas, and the files
generated can be utilised to produce fully working compilers, the tool is great
for learning and for small projects.

While writing the lexer and parser components for the type system markup language
myself might result in a more efficient and complete product, I will instead be
making use of the BNFC tool to create the front end components. This is not only
because of the pros of the tool discussed above, but also due to the timescale
of the project. Attempting to construct the front-end components alongside
completing my other 3rd year modules simply wouldn't be viable. Using the BNFC
tool also provides a more complicated LBNF example for IMPL students to
investigate in future years of study.

\section{Attribute Grammars \& Syntax Directed Translation}
We described type checking in the previous chapter, but how do we
integrate it into the compiler pipeline? One method, conceived by Donald Knuth
is the Attribute Grammar \cite{KnuthGrammars}. An attribute grammar is a
context-free grammar which has been extended to include additional semantic
rules, tied to each non-terminal symbol. These additional rules are known as
attributes. Sebesta provides a nice example of an attribute grammar for an
assignment statement in a simple language, shown in Figure
\ref{fig:AttributeGrammar}.

\begin{figure}
    \begin{minipage}[b]{.5\textwidth}
        \begin{align*}
            Assign &\rightarrow Var \; "=" \; Expr \\
            Expr &\rightarrow Var_2 "+" Var_3 \\
            Expr &\rightarrow Var \\
            Var &\rightarrow A \, | \, B \, | \, C
        \end{align*}
        \subcaption{Production Rules}
        \label{fig:ProductionRules}
    \end{minipage}
    \begin{minipage}[b]{.5\textwidth}
        \begin{align*}
                ExpectedType(Expr) &= ActualType(Var) \\
                ActualType(Expr) &= if ActualType(Var_2 = Int) \&\& \\
                ActualType(Expr) &= ActualType(Var) \\
                ActualType(Var) &= lookupType(Var)
        \end{align*}
        \subcaption{Attribute Definitions}
        \label{fig:AttributeDefinitions}
    \end{minipage}
    \caption{An attribute grammar for an assignment statement}
    \label{fig:AttributeGrammar}
\end{figure}

\section{Applying Category Theory To Type Checking}
\label{sec:CategoryTheory}
Using a functional language like Haskell provides us with the capability to
create elegant abstractions with which we can perform powerful computations
with. By borrowing some ideas from Category Theory, we can enhance our Abstract
Syntax data structures with desirable properties to make type checker generation
easier.

\subsection{Functors}
The first abstraction we will borrow from Category Theory is the Functor. The
use of higher order functions is becoming ever more prevalent in modern
programming languages, especially functions such as \lstinline{map},
\lstinline{filter} and \lstinline{foldr}. The \lstinline{map} function in
particular is incredibly powerful, as it can take a function and a list as
input, and repeatedly apply that function to every element in the list returning
a new list as a result. The following Haskell code provides an implementation
of \lstinline{map}:
\lstset{language=Haskell,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{Orange},
    identifierstyle=\color{MidnightBlue},
    stringstyle=\color{OliveGreen},
    showstringspaces=false,
    frame=single}
\lstinputlisting{code-examples/map.hs}
From looking at the function signature, it can be seen that map only works on
lists. But what if we wanted to apply map to a data structure other than list?
The Functor allows us to do this. A data structure is considered a Functor if
it provides an implementation of the \lstinline{fmap} function, with the
following type signature:
\lstinputlisting{code-examples/fmap.hs}
Here, \lstinline{f} is our data structure. The \lstinline{fmap} function
generalises the map function to some data structure \lstinline{f}. An example
of this concept being used specifically in this project could be...

\textit{I'm working on a suitable example for this section...}

\subsection{Monads}
We can extend the concept of the Functor further to make use of another
abstraction known as a Monad. A Monad is a data structure which implements the
following two operations, described here in Haskell:
\lstinputlisting{code-examples/monad.hs}
Monads provide us with a way of seperating pure and impure computation which
may have side-effects. A very useful application of this is in error handling
and performing operations which may or may not fail. If we find when type
checking that our program fails, we should attempt to diagnose the problem and
report back to the user. A data structure for this very purpose is generated by
the BNFC tool and stored in the {\ttfamily ErrM.hs} file mentioned in Section
\ref{sec:BNFC}, shown below:
\lstinputlisting{code-examples/err.hs}
If the computation is successful then we can return some value of type
{\ttfamily a}, otherwise we return a string which explains the error. The
{\ttfamily >>=} operator provides us with to chain together computations using
the {\ttfamily Err} data type. If at any point the computation produces an
error, we can simply propagate the error message to the end of the computation
chain. Consider the following type checking code:
%\lstinputlisting{code-examples/monad_type_check.hs}

\textit{Working on a do-notation example demonstrating the Err data structure...}

\subsection{Foldables}

\cite{MilewskiCTFP}
\cite{HuttonHaskell}

\section{Advanced Type Rules \& Context}
With attribute grammars annotating the program abstract syntax tree with type
information using attributes, an alternative approach involves processing the
abstract syntax tree using pattern matching functions in a functional programming
language like Haskell. These functions would implement type rules like those
described in Section \ref{sec:Chap1TypeSystems} and \ref{sec:Chap1TypeChecking}.
To begin thinking about how we might construct these functions, we should first
consider what form they might take, and whether the form is predictable enough
to generate.

\subsection{Representing Simple Type Rules in a Programming Language}
Consider the following type rule:
\begin{displaymath}
    \prftree[l]{||}{\Gamma \vdash a:\mathbb{B}}{\Gamma \vdash b:\mathbb{B}}{
    \Gamma \vdash a \, || \, b:\mathbb{B}}
\end{displaymath}
Rules like the one above are easy to translate into
Haskell and seem to have a predictable form which can be generated. Consider
the following Haskell source code:
\lstinputlisting{code-examples/and_type_rule.hs}

The consequents of the rule appear as arguments to the check function on the
left hand side, and the antecedents appear as the function body on the right
hand side. The TrueLiteral and FalseLiteral rules return \lstinline{Ok True}
because they are axioms and have no antecedents. Type rules which don't mutate
the program context should be easy to generate because they don't require us
to lookup the types of any variables. The complexity of our type rule
implementations will begin to develop when we need to start considering the
program context when type checking.

\subsection{Context, Environment \& Scoping}
If we are type checking an expression that is incredibly simple and only uses
literal values such as True, False and 42, then we won't need to consult the
program context. Unfortunately, programming languages which only make use of
literal types don't gain much traction in the real world because they aren't
very useful. Modern languages make use of more powerful language concepts such
as variables and functions, concepts which require us to keep track of
declarations and definitions in order to type check our programs correctly.
This is precisely what the context structure is for, to store variable-type
pairs or function type-pairs.

The design and behaviour of the context data structure used for storing these
bindings is largely dictated by the scoping rules of the programming language.
If the language decided to make use of static scoping, then the contexts
structures may be pushed and popped from a context stack as new blocks are
entered and exited. If dynamic scoping were used instead, then a different
implementation would be required. The designer of the type system would need
to describe the behaviour of the context data structure in the type system
language to make the type checker generator as flexible as possible.

Ranta \cite{Ranta} proposes a model for contexts appropriate for a statically
scoped langauge in the form of the following Haskell source code:
\lstinputlisting{code-examples/context.hs}
The code describes the context as a pairing of a hash map for mapping function
names to their corresponding input and output types, and a list of hash maps
mapping variable names to types. The function declarations remain global and can
be seen as they are acquired by the type checker. The contexts can be pushed
and popped from the context stack as they are encountered when traversing the
parse tree.

\subsection{More Advanced Type Rules}
Now consider the following type rule for type checking a variable, and its
corresponding code:
\begin{displaymath}
    \prftree[l,r]{[if $v:T$ in $\Gamma$]}{VarT}{\Gamma \vdash v:T}
\end{displaymath}

\lstinputlisting{code-examples/var_type_rule.hs}
This rule seems more complicated to express in a programming language, as it
requires us to check whether we have a variable binding in our context which
matches the variable we are checking. Looking up a variable would be considered
a side-condition in a type rules, and it seems that any rules which have
side-conditions will be harder to generate.

Here are some examples of rules with different side-conditions, as documented
by Pierce \cite{PierceTAPL}:

\begin{displaymath}
\prftree[l, r]{[if $f:(T_1, \dots, T_n) \rightarrow T$ in $\Gamma$]}{FuncCallT}{\Gamma \vdash a_1:T_1}{\dots}{\Gamma \vdash a_n:T_n}{\Gamma \vdash f(a_1, \dots, a_n):T}
\end{displaymath}
\begin{displaymath}
\prftree[l, r]{[if Pointer T]}{LocationT}{\Gamma \vdash l:Pointer \, T}
\end{displaymath}

\textit{This section will be fleshed out to include type rules for more complex
    types such as pairs, product types, records, sum types, lists,
    pointers, etc...}



\chapter{Problem Analysis}
\section{Project Requirements}
The sucess of this project is measured by how many different type rules are
expressible in the type system language, and how well the generator is able to
implement those rules in the form of a type checker.

\section{Software Component Analysis}
The end product we are creating requires us to take into account many different
software components. To better understand these components and how they depend
on each other, we can construct an component diagram to help us visualise
the relationships. The diagram is shown in Figure \ref{fig:SoftwareComponents}.
The components this project focuses on are highlighted in red. This chapter
will focus on the development of the Type Language Specification and the
Type Rule Lexer/Parser generated from it, while Chapter 5 will focus on the
Type Checker Generator.

\begin{figure}[!ht]
    \begin{tikzpicture}
        \node (SLSC) [label] {\footnotesize Source Language Source Code};
        \node (TLS) [label, above of=SLSC, color=red, yshift=2.25cm] {\footnotesize Type System Language Specification};
        \node (SLL/P) [archbox, right of=SLSC, xshift=3cm] {\footnotesize Source Language Lexer/Parser};
        \node (SLTC) [archbox, right of=SLL/P, xshift=3cm] {\footnotesize Source Language Type Checker};
        \node (TCG) [archbox, above of=SLTC, yshift=0.75cm, color=red] {\footnotesize Type Checker Generator};
        \node (BNFC) [archbox, above of=SLL/P, yshift=2.25cm] {\footnotesize BNFC};
        \node (TCGL/P) [archbox, above of=TCG, yshift=0.5cm, xshift=1cm, color=red] {\footnotesize Type Rule Lexer/Parser};
        \node (CBE) [dashedarchbox, right of=SLTC, xshift=3cm] {\footnotesize Compiler Backend};
        \node (TR) [label, above of=TCGL/P, yshift=0.5cm] {\footnotesize Type Rule Specification};
        \node (SLS) [label, above of=BNFC, yshift=0.5cm] {\footnotesize Source Language Specification};

        \draw [arrow] (SLS) -- (BNFC);
        \draw [arrow] (TR) -- (TCGL/P);
        \draw [arrow] (SLSC) -- (SLL/P);
        \draw [arrow] (TLS) -- (BNFC);
        \draw [arrow] (BNFC) -- node [anchor=east] {\scriptsize generates}(SLL/P);
        \draw [arrow] (BNFC) -- node [anchor=south] {\scriptsize generates}(TCGL/P);
        \draw [arrow] (SLL/P) -- (SLTC);
        \draw [arrow] (TCG) -- node [anchor=west] {\scriptsize generates} (SLTC);
        \draw [arrow] (TCGL/P) -- (TCG);
        \draw [arrow] (SLTC) -- (CBE);
        \draw [arrow] (SLS) .. controls +(up:4cm) and +(right:12cm) .. (TCG);
    \end{tikzpicture}

    \caption{A diagram relating the various components of the source language with
        the type system language}
    \label{fig:SoftwareComponents}
\end{figure}

\chapter{Type System Language}

\section{Initial Language Design}
When designing the type system language, we first considered what we might
want the language to look like. The initial idea was to create a language which
mirrored the mathematical notation for describing type rules we have presented
throughout the project. This resulted in an initial plaintext language
demonstrated in Figure \ref{fig:InitialTypeLanguage}. This language seemed to
be able to express a large number of type rules, but upon further investigation
a number of flaws were exposed. The main issue with this initial draft was that
it created a direct dependency on the source language we were describing type
rules for. The consequences of this would result in a new type system language
needing to be constructed before we described a new source language. This would
require a large amount of work from the user, and would also expose the type
system language implementation, requiring the user to understand more than
should be assumed.

\begin{figure}
\lstset{basicstyle=\footnotesize\ttfamily, frame=single,
    language=,}
\lstinputlisting{code-examples/TypeSpecification1.txt}
\caption{Type rules for variables and addition written in the initial type
    description language}
\label{fig:InitialTypeLanguage}
\end{figure}

\section{Eliminating Source Language Dependency}
After much thinking and consideration, a way of disconnecting the dependency
between the source language and type system language was found. For this idea
to work, we must assume that the user has created a BNFC specification for their
source language and has successfully used the BNFC tool to produce the output
files described in Section \ref{sec:BNFC}, which is reasonable. The solution
involves encoding every instance of source language used in the type rule
specification as strings, and using the parsing functions produced by BNFC to
acquire the AST for those snippets of source language. These ASTs can then be
merged with the AST produced by the Type System Language to provide the full
type rule descriptions, without depending on the source language itself. The
parsing functions will be called in the Type Checker Generator. Examples in the
revised language are shown in Figure \ref{fig:RevisedTypeLanguage}.

\begin{figure}
\lstset{basicstyle=\footnotesize\ttfamily, frame=single,
    language=,}
\lstinputlisting{code-examples/TypeSpecification2.txt}
\caption{Type rules for variables and addition written in the revised type
    description language without the source language dependency}
\label{fig:RevisedTypeLanguage}
\end{figure}


\section{Adding a Context Description System}

\section{Adding Support for Different Side-Conditions}

\chapter{Type Checker Generation}

\section{Generating Basic Type Checkers}

\chapter{Results and Evaluation}

\printbibliography
\end{document}
